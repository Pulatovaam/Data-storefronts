<h1 align="center"> Создание витрин данных для аналитиков

## Цель моей работы:      
#### Создать витрину данных для аналитиков, которая будет содержать агрегированные метрики по карточкам товаром, популярным категориям и прочим.       
## Состав проекта:        
- реализация задачи на Spark, и один DAG Airflow       
- создание запросов о построении Greenplum - витрины, исполняемого из DAG Airflow

## Необходимые технологии:
- Python (библиотеки для работы с данными, такие как Pandas, PySpark)       
- SQL (для работы с базами данных)       
- ETL-инструменты (например, Apache Airflow)       
- Облачные решения (AWS, GCP или аналогичные)       
- Знание принципов работы с большими данными (Big Data)

![изображение_2025-02-06_175055981](https://github.com/user-attachments/assets/a8fa62c4-f6a8-4cbe-bdc9-ef9d1571ad8b) 


## Задачи проекта: 
**Задача 1**.  
* **Необходимо создать pipeline обработки данных (DAG Airflow), который реализует все запросы из последующих шагов.**.  
* **Требования к Airflow DAG:**
1. DAG не имеет расписания (schedule_interval=None)      
2. Каждая Spark задача выполняется в своей таске (отдельный submit)       
3. Все таски выполняются параллельно     
**Задача 2. Обработка данных для отчета по LineItems**
* **Необходимо обогатить исходные данные дополнительными параметрами и агрегатами на основе показателей.**
  ![1750671768933](https://github.com/user-attachments/assets/e62e0fcc-fb57-4fcf-b8b8-0a6192a7d63c)
**Задача 3.Пайплайны витрин DWH**
* **Все таблицы должны создаваться в вашей схеме**
* **Создание таблиц должно происходить с помощью оператора SQLExecuteQueryOperator**
* **Каждый оператор должен вызываться последовательно после построения отчета и сохранения его в вашем S3.**
* **Описание требуемых структур таблиц представлено в последующих шагах.**
* **Создание таблиц должно быть идемпотентным.**
**Задача 4. Витрины**
  * **Для витрин нужно использовать Greenplum PXF**
  * **Витрина SellerItems.**
* Добавьте шаг items_datamart в DAG Airflow для создания внешней таблицы над вашими данными размещенными в S3.
* Таблица должна называться seller_items и размещена в вашей схеме.
**Создание View**
  * **View "Не надежные продавцы"**
  Добавьте шаг create_unreliable_sellers_report_view в DAG Airflow для создания view таблицы с перечнем ненадежных продавцов:
1. Если товар выставлен на торговую площадку более 100 дней. 
2. Количество товаров размещенных на складе больше, чем количество заказанных товаров.
3. В расчете учитываются все позиции предоставленные продавцов на площадке.
 * **View "Отчет брендов"**
 Добавьте шаг create_brands_report_view в DAG Airflow для создания view таблицы с отчетом по брендам.      
![1750672537924](https://github.com/user-attachments/assets/0b2ac442-120a-4cb9-a0e0-0a325bcc3341)


